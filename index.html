<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-7580334-2');
  </script>

  <title>Wenhu Chen</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  
  <meta name="author" content="Wenhu Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="style/stylesheet.css">
  <link rel="stylesheet" type="text/css" href="style/orig.css" media="all" />
  <link rel="icon" type="image/png" href="https://upload.wikimedia.org/wikipedia/en/thumb/6/6e/University_of_Waterloo_seal.svg/300px-University_of_Waterloo_seal.svg.png">
</head>

<body class="blue light">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:10px;">
    <tbody>
      <tr style="padding:0px">
      <td style="padding:0px">
        <ul class="menu">
            <li class="active"><a href="index.html">Home</a></li>
            <li><a href="publication.html">Publication</a></li>
            <li><a href="teaching.html">Teaching</a></li>
            <li><a href="lab.html">TIGER Lab</a></li>     
            <li><a href="talks.html">Talks</a></li>                
            <li><a href="miscellaneous.html">Miscellaneous</a></li>                
        </ul>

        <hr>
        
        <h3>Wenhu Chen [陈文虎 in Chinese]</h3>
        <br>
        <a href="images/headshot.jpg"><img style="width:200px;max-width:50%" alt="profile photo" src="images/headshot.jpg" class="hoverZoomLink"></a>
        <h4>Assistant Professor at Computer Science at University of Waterloo</h4>
        <h4>Vector Institute, CIFAR AI Chair</h4>
        <h4>Senior Research Scientist at Google Deepmind (20% Part-time)</h4>
        <p>Email: wenhuchen [at] uwaterloo [dot] ca </p>
        <p>
        <a href="https://scholar.google.com/citations?user=U8ShbhUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
        <a href="images/CV_WENHU_2025_Jan.pdf">CV (updated in Jan 25)</a> &nbsp/&nbsp
        <a href="https://github.com/wenhuchen">Github</a> &nbsp/&nbsp
        <a href="https://twitter.com/WenhuChen">Twitter</a>
        </p>

        <hr>

        <h3>Biography</h3>
        Wenhu Chen has been an assistant professor at Computer Science Department in University of Waterloo and Vector Institute since 2022. He obtained Canada CIFAR AI Chair Award in 2022. He also works for Google Deepmind as a part-time research scientist since 2021. Before that, he obtained his PhD from the University of California, Santa Barbara under the supervision of William Wang and Xifeng Yan. His research interest lies in natural language processing, deep learning and multimodal learning. He aims to design models to handle complex reasoning scenarios like math problem-solving, structure knowledge grounding, etc. He is also interested in building more powerful multimodal models to bridge different modalities. He won the prestigious Golden Jubilee Research Excellence Award in Waterloo in 2025. He received the Area Chair Award in AACL-IJCNLP 2023, the Best Paper Honorable Mention in WACV 2021, and the UCSB CS Outstanding Dissertation Award in 2021.
        <hr>

        <h3>Research Interest</h3>
        My research interest covers the following aspects:
        <ul>
            <li> Reasoning</li>
            <li> Controllable GenAI</li>
            <li> Information Retrieval</li>
            <li> Benchmarks and Evaluation</li>
        </ul>

        <h3> Research Highlights </h3>
        You might have heard of me because of the following work I conducted.
        <h4> 1. Natural Language Processing (LLMs) </h4>
        <ul>
            <li> <a href="https://arxiv.org/abs/2309.05653">MAmmoTH</a>/<a href="https://arxiv.org/abs/2405.03548">MAmmoTH2</a>/<a href="https://arxiv.org/abs/2501.17703">Critique Fine-Tuning</a>: Advancing reasoning model to solve complex reasoning tasks</li>
            <li> <a href="https://arxiv.org/abs/2211.12588">Program-of-Thoughts</a>: A prompting strategy to use tools to sovle complex reasoning tasks</li>
            <li> <a href="https://arxiv.org/abs/2402.14658">OpenCoderInterpreter</a>/<a href="https://arxiv.org/abs/2502.01718">AceCoder</a>: Advanced coding language models for complex tasks</li>
            <li> <a href="https://arxiv.org/abs/2405.19327">MAP-Neo</a>/<a href="https://huggingface.co/datasets/m-a-p/FineFineWeb">Fine-FineWeb</a>: Fully open-source language models with high-quality pre-training datasets</li>
            <li> <a href="https://arxiv.org/abs/2305.01750">KB-BINDER</a>/<a href="https://arxiv.org/abs/2210.06710">TableCoT</a>/<a href="https://arxiv.org/abs/2402.16671">StructLM</a>: Grounding foundation models on structured knowledge </li>
        </ul>

        <h4> 2. Multimodal Understanding (Image + Video) </h4>
        <ul>
            <li> <a href="https://arxiv.org/abs/2210.02928">MuRAG</a>/<a href="https://arxiv.org/abs/2311.17136">UniIR</a>/<a href="https://arxiv.org/abs/2403.19651">MagicLens</a>/<a href="https://arxiv.org/abs/2410.05160">VLM2Vec</a>/<a href="https://arxiv.org/abs/2406.11251">DSE</a>/<a href="https://arxiv.org/abs/2412.14457">VISA</a>: the framework to enable unified and compositional multimodal information retrieval</li>
            <li> <a href="https://arxiv.org/abs/2405.01483">Mantis</a>/<a href="https://arxiv.org/abs/2412.05237">MAmmoTH-VL</a>/<a href="https://arxiv.org/abs/2503.10582">MAmmoTH-VL2</a>: advanced vision-language models with better reasoning skills</li>
            <li> <a href="https://arxiv.org/abs/2412.00927">VISTA</a>/<a href="https://arxiv.org/abs/2503.11579">Vamba</a>: Long video understanding models</li>
        </ul>

        <h4> 3. Multimodal Generation (Image + Video) </h4>
        <ul>
            <li> <a href="https://arxiv.org/abs/2209.14491">Re-Imagen</a>/<a href="https://arxiv.org/abs/2304.00186">SuTI</a>/<a href="https://arxiv.org/abs/2310.02992">Kosmos-G</a>/<a href="https://arxiv.org/abs/2401.01952">Instruct-Imagen</a>: the most effective and efficient and controllable image generation models</li>
            <li> <a href="https://arxiv.org/abs/2405.18750">T2V-Turbo</a>/<a href="https://arxiv.org/abs/2410.05677">T2V-Turbo-v2</a>: efficient text-to-video generation models</li>
            <li> <a href="https://arxiv.org/abs/2306.10012">MagicBrush</a>/<a href="https://arxiv.org/abs/2411.07199">OmniEdit</a>/<a href="https://arxiv.org/abs/2403.14468">AnyV2V</a>: powerful image and video editing models</li>
        </ul>

        <h4> 4. Benchmarks & Evaluation </h4>
        <ul>
            <li> <a href="https://arxiv.org/abs/2311.16502">MMMU</a>/<a href="https://arxiv.org/abs/2406.01574">MMLU-Pro</a>/<a href="https://arxiv.org/abs/2305.12524">TheoremQA</a>/<a href="https://arxiv.org/abs/2410.10563">MEGA-Bench</a>:: the commonly used language model and vision-language model evaluation suite</li>
            <li> <a href="https://arxiv.org/abs/1909.02164">TabFact</a>/<a href="https://arxiv.org/abs/2004.07347">HybridQA</a>/<a href="https://arxiv.org/abs/2010.10439">OTT-QA</a>: Table and text reasoning evaluation benchmarks</li>
        </ul>

        <h4> 5. Others </h4>
        <ul>
            <li> <a href="https://arxiv.org/abs/2306.00107">MERT</a>/<a href="https://arxiv.org/abs/2402.16153">ChatMusician</a>/<a href="https://arxiv.org/abs/2503.08638">YuE</a>: Foundation models for understanding and composing music</li>
            <li> <a href="https://arxiv.org/abs/2502.19400">TheoremExplainAgent</a>: Building agents for composing education videos</li>
        </ul>
        
        <h3>TIGER Lab</h3>
        <p> I direct the <a href="lab.html">Text and Image GEnerative Research (TIGER) lab</a>. My lab is focused on studying different generative models in different modalities including text, images, videos and music. We are committed to building powerful state-of-the-art models for various domains. Our lab is always looking for talented and self-motivated students.</p>

        <h3> Awards </h3>
        <ul>
            <li> 2025: Math Golden Jubilee Award </li>
            <li> 2024: CVPR Best Paper Finalist </li>
            <li> 2023: AACL-IJCNLP23 Area Chair Award </li>
            <li> 2022: Canada CIFAR AI Chair </li>
            <li> 2021: UCSB CS Outstanding Dissertation Award </li>
            <li> 2021: WACV21 Best Student Paper Honorable Mention </li>
            <li> 2018: Tencent Rhino-Bird Award</li>
            <li> 2016: IDEA Research Grant</li>
        </ul>

        <h3> Fundings </h3>
        <ul>
            <li> CIFAR AI Chair Funding: Accessing Diverse Web Knowledge with Natural Language Interface (2022 - 2027) </li>
            <li> NSERC Discovery Fund: Building Semiparametric Models to Decouple Knowledge from Computation (2023 - 2028)  </li>
            <li> Mitacs Accelerate Fund: Question Answering over Long Clinical Documents (2024 - 2026) </li>
            <li> CIFAR AI Catalyst Fund: Generating Images with Multimodal Instruction (2024 - 2026) </li>
            <li> National Research Council Canada - AI4D Funding: Accelerating Scientific Discovery with Foundation Models (2024 - 2026) </li>
            <li> National Research Council Canada - New Beginning Funding: Building More Efficient Visual Generative Models (2025 - 2026) </li>
        </ul>

        <!--<h3> Media Coverage </h3>
        <ul>
            <li><a href="https://cs.uwaterloo.ca/news/wenhu-chen-professor-studies-nlp-dl-knowledge-representation-reasoning">Waterloo Interview</a></li>
            <li><a href="https://vectorinstitute.ai/vector-faculty-member-wenhu-chen-gives-an-overview-of-his-labs-work-around-improving-and-benchmarking-foundation-models">Vector Blog</a></li>
            <li><a href="https://uwaterloo.ca/artificial-intelligence-institute/news/wenhu-chen-and-xi-he-named-canada-cifar-ai-chairs">CIFAR Award</a></li>
            <li><a href="https://www.marktechpost.com/2024/05/16/tiger-lab-introduces-mmlu-pro-dataset-for-comprehensive-benchmarking-of-large-language-models-capabilities-and-performance/">MarkTechPost (MMLU-Pro)</a></li>
        </ul>-->

        <hr>
        
        <div class="container">
          <div align="center">
               <a href="https://clustrmaps.com/site/1bibk" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=eJ_psElkiW3GTpiD6PIeBSESH3qlWZZips-5IZh2Efk&cl=ffffff"></a>
          </div>
        </div>
    </td>
    </tr>
    </tbody>
  </table>
</body>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-131560165-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

</body>

</html>
