<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-7580334-2');
  </script>

  <title>Wenhu Chen</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  
  <meta name="author" content="Wenhu Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="style/stylesheet.css">
  <link rel="stylesheet" type="text/css" href="style/orig.css" media="all" />
  <link rel="icon" type="image/png" href="https://upload.wikimedia.org/wikipedia/en/thumb/6/6e/University_of_Waterloo_seal.svg/300px-University_of_Waterloo_seal.svg.png">
</head>

<body class="blue light">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:10px;">
    <tbody>
      <tr style="padding:0px">
      <td style="padding:0px">
        <ul class="menu">
            <li class="active"><a href="index.html">Home</a></li>
            <li><a href="publication.html">Publication</a></li>
            <li><a href="teaching.html">Teaching</a></li>
            <li><a href="lab.html">TIGER Lab</a></li>     
            <li><a href="talks.html">Talks</a></li>                
            <li><a href="miscellaneous.html">Miscellaneous</a></li>                
        </ul>

        <hr>
        
        <h3>Wenhu Chen [陈文虎 in Chinese]</h3>
        <br>
        <a href="images/headshot.jpg"><img style="width:200px;max-width:50%" alt="profile photo" src="images/headshot.jpg" class="hoverZoomLink"></a>
        <h4>Researcher in LLM/Agents/Multimodality</h4>
        <p>Email: wenhuchen [at] uwaterloo [dot] ca | hustchenwenhu [at] gmail [dot] com </p>
        <p>
        <a href="https://scholar.google.com/citations?user=U8ShbhUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
        <a href="images/CV_WENHU.pdf">CV (updated in July 25)</a> &nbsp/&nbsp
        <a href="https://github.com/wenhuchen">Github</a> &nbsp/&nbsp
        <a href="https://twitter.com/WenhuChen">Twitter</a>
        </p>

        <hr>

        <h3>Biography</h3>
        Wenhu Chen is an assistant professor at the University of Waterloo (on Leave). He obtained the Canada CIFAR AI Chair Award in 2022. He worked at Google DeepMind from 2021 to 2025, where he contributed to the Gemini multimodel and evaluation efforts. Before that, he obtained his PhD from the CS department of the University of California, Santa Barbara. His research interest lies in natural language processing, deep learning, and multimodal learning. He aims to design models that handle complex reasoning scenarios, such as math problem-solving and knowledge grounding. He is also interested in building more powerful multimodal models to bridge different modalities. He won the prestigious Golden Jubilee Research Excellence Award at the University of Waterloo in 2025. His won the best paper award at TMLR 2025. He received the Area Chair Award in AACL-IJCNLP 2023, the Best Paper Honorable Mention in WACV 2021, and the UCSB CS Outstanding Dissertation Award in 2021.
        <hr>

        <h3>Research Interest</h3>
        <ul>
            <li> Reasoning</li>
            <li> Information Retrieval</li>
            <li> Benchmarks and Evaluation</li>
            <li> Generative AI</li>
        </ul>

        <h3> Research Highlights </h3>
        <h4> 1. LLM Reasoning </h4>
        <ul>
            <li> <a href="https://arxiv.org/abs/2211.12588">Program-of-Thoughts</a>: A prompting strategy to use tools to solve complex reasoning tasks</li>
            <li> <a href="https://arxiv.org/abs/2309.05653">MAmmoTH</a>/<a href="https://arxiv.org/abs/2405.03548">MAmmoTH2</a>/<a href="https://arxiv.org/abs/2505.14652">General Reasoner</a>: Advancing reasoning model to solve complex reasoning tasks</li>
            <li> <a href="https://arxiv.org/abs/2402.14658">OpenCoderInterpreter</a>/<a href="https://arxiv.org/abs/2502.01718">AceCoder</a>: Advanced coding language models for complex tasks</li>
        </ul>

        <h4> 2. LLM Agents </h4>
        <ul>
            <li><a href="https://arxiv.org/abs/2509.01055">Verl-Tool</a>: A holistic training framework for diversetool use</li>
            <li><a href="https://arxiv.org/abs/2509.06501">WebExplorer</a>: A SoTA open-sourced deep research model</li>
            <li><a href="https://arxiv.org/abs/2508.06600">BrowseComp-Plus</a>: A commonly used benchmark for deep research models</li>
        </ul>
        
        <h4> 3. Multimodal Understanding </h4>
        <ul>
            <li> <a href="https://arxiv.org/abs/2405.01483">Mantis</a>/<a href="https://arxiv.org/abs/2412.05237">MAmmoTH-VL</a>: advanced vision-language models with better reasoning skills</li>
            <li> <a href="https://arxiv.org/abs/2504.08837">VL-Rethinker</a>/<a href="https://arxiv.org/abs/2505.15966">Pixel-Reasoner</a>: advanced vision-language models with better reasoning skills</li>
            <li> <a href="https://arxiv.org/abs/2410.05160">VLM2Vec</a>/<a href="https://arxiv.org/abs/2507.04590">VLM2Vec-V2</a>: the framework to enable unified and compositional multimodal information retrieval</li>
        </ul>

        <h4> 4. Multimodal Generation </h4>
        <ul>
            <li> <a href="https://arxiv.org/abs/2209.14491">Re-Imagen</a>/<a href="https://arxiv.org/abs/2304.00186">SuTI</a>/<a href="https://arxiv.org/abs/2401.01952">Instruct-Imagen</a>: the most effective and efficient and controllable image generation models</li>
            <li> <a href="https://arxiv.org/abs/2405.18750">T2V-Turbo</a>/<a href="https://arxiv.org/abs/2410.05677">T2V-Turbo-v2</a>: efficient text-to-video generation models</li>
            <li> <a href="https://arxiv.org/abs/2306.10012">MagicBrush</a>/<a href="https://arxiv.org/abs/2411.07199">OmniEdit</a>/<a href="https://arxiv.org/abs/2403.14468">AnyV2V</a>/<a href="https://arxiv.org/abs/2510.08377">UniVideo</a>: powerful image and video editing models</li>
        </ul>

        <h4> 5. Benchmarks & Evaluation </h4>
        <ul>
            <li> <a href="https://arxiv.org/abs/2311.16502">MMMU</a>/<a href="https://arxiv.org/abs/2406.01574">MMLU-Pro</a>/<a href="https://arxiv.org/abs/2410.10563">MEGA-Bench</a>: the commonly used language model and vision-language model evaluation suite</li>
        </ul>

        <h4> 6. Others </h4>
        <ul>
            <li> <a href="https://arxiv.org/abs/2306.00107">MERT</a>/<a href="https://arxiv.org/abs/2402.16153">ChatMusician</a>/<a href="https://arxiv.org/abs/2503.08638">YuE</a>: Foundation models for understanding and composing music</li>
            <li> <a href="https://arxiv.org/abs/2405.19327">MAP-Neo</a>/<a href="https://huggingface.co/datasets/m-a-p/FineFineWeb">Fine-FineWeb</a>: Fully open-source language models with high-quality pre-training datasets</li>
            <li> <a href="https://arxiv.org/abs/2507.06261">Gemini-2.5</a>: Contributed to the Gemini-2.5 Model at Google DeepMind. </li>
            <li> <a href="https://arxiv.org/abs/1909.02164">TabFact</a>/<a href="https://arxiv.org/abs/2004.07347">HybridQA</a>/<a href="https://arxiv.org/abs/2010.10439">OTT-QA</a>: Table and text reasoning evaluation benchmarks</li>
        </ul>
        
        <h3>TIGER Lab</h3>
        <p> I direct the <a href="lab.html">Text and Image GEnerative Research (TIGER) lab</a>. My lab is focused on studying different generative models in different modalities including text, images, videos and music. We are committed to building powerful state-of-the-art models for various domains. Our lab is always looking for talented and self-motivated students.</p>

        <h3> Awards </h3>
        <ul>
            <li> 2025: Outstanding Paper Award at TMLR 2025 </li>
            <li> 2025: Math Golden Jubilee Award </li>
            <li> 2024: CVPR Best Paper Finalist </li>
            <li> 2023: AACL-IJCNLP23 Area Chair Award </li>
            <li> 2022: Canada CIFAR AI Chair </li>
            <li> 2021: UCSB CS Outstanding Dissertation Award </li>
            <li> 2021: WACV21 Best Student Paper Honorable Mention </li>
        </ul>

        <h3> Fundings and Grants</h3>
        <ul>
          <li>
            CIFAR AI Chair Award
            <ul>
              <li>Title: Accessing Diverse Web Knowledge with Natural Language Interface</li>
              <li>Years: 2022 - 2027</li>
              <li>Amount: 725,000 CAD</li>
            </ul>
          </li>
        
          <li>
            NSERC Discovery
            <ul>
              <li>Title: Building Semiparametric Models to Decouple Knowledge from Computation</li>
              <li>Years: 2023 - 2028</li>
              <li>Amount: 12,500 CAD</li>
            </ul>
          </li>
        
          <li>
            Mitacs Accelerate
            <ul>
              <li>Title: Question Answering over Long Clinical Documents</li>
              <li>Years: 2024 - 2026</li>
              <li>Amount: 90,000 CAD</li>
            </ul>
          </li>
        
          <li>
            CIFAR AI Catalyst
            <ul>
              <li>Title: Generating Images with Multimodal Instruction</li>
              <li>Years: 2024 - 2026</li>
              <li>Amount: 100,000 CAD</li>
            </ul>
          </li>
        
          <li>
            National Research Council Canada - AI4D Funding
            <ul>
              <li>Title: Accelerating Scientific Discovery with Foundation Models</li>
              <li>Years: 2024 - 2026</li>
              <li>Amount: 118,000 CAD</li>
            </ul>
          </li>
        
          <li>
            National Research Council Canada - New Beginning
            <ul>
              <li>Title: Building More Efficient Visual Generative Models</li>
              <li>Years: 2025 - 2026</li>
              <li>Amount: 25,000 CAD</li>
            </ul>
          </li>
        
          <li>
            CIAFR Solution Network - Safer AI for the Global South
            <ul>
              <li>Title: A Citizen-Centered Co-Creating Dialect Bias Benchmarks, Mitigation Tools, and Policy Solutions</li>
              <li>Years: 2026 - 2028</li>
              <li>Amount: 700,000 CAD</li>
            </ul>
          </li>
        
          <li>
            CFI-JELF
            <ul>
              <li>Title: Enriching the linguistic diversity of open language models</li>
              <li>Years: 2026 - 2030</li>
              <li>Amount: 336,000 CAD</li>
            </ul>
          </li>
        </ul>


        <hr>
        
        <div class="container">
          <div align="center">
               <a href="https://clustrmaps.com/site/1bibk" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=eJ_psElkiW3GTpiD6PIeBSESH3qlWZZips-5IZh2Efk&cl=ffffff"></a>
          </div>
        </div>
    </td>
    </tr>
    </tbody>
  </table>
</body>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-131560165-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

</body>

</html>
